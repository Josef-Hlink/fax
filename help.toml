[PATHS]
iso = "Path to your Melee ISO file."
exe = "Path to your headless Dolphin executable."
zips = "List of paths to your .zip files containing .slp files (supports glob patterns)."
slp = "Path to your directory of (unzipped) .slp files."
sql = "Path where the SQLite database file for indexing .slp files is stored."
mds = "Home directory for your .mds datasets."
logs = "Home directory for Loguru logs."
weights = "Home directory for best performing model weights saved during training."
replays = "Home directory for replays generated by running evaluation games."
dolphin-home = "Used for Dolphin Emulator settings and memory cards."  # TODO: update
[BASE]
seed = "Random seed that will be set at the start of anything involving stochasticity."
debug = "If true, will run in debug mode for more verbose logging."
wandb = "If true, will log training metrics to Weights & Biases."
n-gpus = "Number of GPUs to use for training. Set to 0 for CPU training."  # TODO: support >1
[MODEL]
n-layers = "Number of transformer layers."
n-heads = "Number of attention heads in each transformer layer."
seq-len = "Number of contiguous frames in each training sample."
emb-dim = "Embedding dimension."  # TODO: has to be same as seq-len?
dropout = "Dropout rate."
gamma = "Discount factor for future rewards in closed-loop evaluation."
[OPTIM]
lr = "Learning rate."  # TODO: refer to docs for these params
wd = "Weight decay."
b1 = "Adam optimizer beta1 parameter."
b2 = "Adam optimizer beta2 parameter."
[TRAINING]
batch-size = "Number of samples to be propagated through the network at once during training and evaluation."
n-epochs = "Number of epochs to train for. Validation happens at the end of each epoch."
n-samples = """
    Number of training samples to draw from the dataset for each epoch.
    This parameter is also used to determine the number of .slp files to load from the zip archives,
    this way we sample exactly one sequence from each episode (replay).
"""
n-val-samples = "Number of samples to use for validation at the end of each epoch."
n-dataworkers = "Number of subprocesses MosaicML will use to load data."
matchup = "Type of data the agent will be trained on. Options are FvF, FvX, XvF, XvX."
n-finetune-epochs = "Number of finetuning epochs to perform after the main training is complete."
finetune-lr-frac = "Fraction of the original learning rate to initialize the finetuning learning rate."
[EVAL]
p1-type = "Type of player 1. Format like <n-epochs>-<matchup>[-ft]. E.g. 1k-FvF-ft for a finetuned agent after 1k epochs of training on FvF data."
p2-type = "Type of player 2. Same format as p1-type, or 'cpu' for the default CPU AI."  # TODO: actually support PvP
n-loops = "Number of times to run the \"infinite\" evaluation loop."
